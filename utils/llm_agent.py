from transformers import AutoTokenizer

from textarena.core import Agent
from textarena.agents.basic_agents import STANDARD_GAME_PROMPT

from vllm import LLM, SamplingParams

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# MODEL_PATH = "/data/models/Qwen3-4B"
MODEL_PATH = "/data/models/Qwen2.5-7B-Instruct"

MODEL = LLM(model=MODEL_PATH, tensor_parallel_size=2)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

from utils.random_agent import clean_obs

def post_processing(response: str) -> str:
    """
    Post-process the response from the LLM to ensure it is in the correct format.
    This function can be customized based on the specific requirements of the game.
    :param response: The raw response string from the LLM.
    :return: The processed string.

    input example:
    ``` plaintext
    <think> Your thoughts and reasoning </think>
    \\boxed{[ACTION]}
    ```
    return: [ACTION]
    """
    # Extract the action from the response
    if "\\boxed{" in response and "}" in response:
        action = response.split("\\boxed{")[-1].split("}")[0].strip()
        return action
    else:
        logger.debug("Response format is incorrect: %r", response)
        return response


def call_llm(messages: list[dict], sampling_params, thinking: bool = False) -> str:
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=thinking,
    )

    response = MODEL.generate(
        [text],
        sampling_params=sampling_params,
        use_tqdm=False
    )
    output = response[0].outputs[0].text
    return output.strip()


class VLLMAgent(Agent):
    def __init__(self, temperature: float = 0.7, top_p: float = 0.95, max_tokens: int = 512):
        super().__init__()
        self.system_prompt = STANDARD_GAME_PROMPT

        self.sampling_params = SamplingParams(
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens
        )

    def __call__(self, observation: str) -> str:
        """
        Call the agent with an observation and return the action.
        :param observation: The observation string from the environment.
        :return: The action string generated by the agent.
        """
        logger.debug("%s Observation: %r", str(self), clean_obs(observation))

        messages = [
            {"role": "system", "content": self.system_prompt}, 
            {"role": "user", "content": observation}
        ]
        response = call_llm(messages, self.sampling_params, thinking=True)

        logger.debug("%s Action: %r", str(self), response)

        return post_processing(response)
    
    def __str__(self):
        return "VLLMAgent"
    

'''
You are an expert Kuhn Poker player.

[Game Rules]
- Kuhn Poker uses a 3-card deck with J, Q, K (J lowest, K highest).
- Each player antes 1 chip and receives 1 card each round (note that the cards are dealt without replacement, so you cannot have the same card as your opponent).
- The player with the highest card wins the pot.

[Action Rules]
- [check]: Pass without betting (only if no bet is on the table)
- [bet]: Add 1 chip to the pot (only if no bet is on the table)
- [call]: Match an opponent's bet by adding 1 chip to the pot
- [fold]: Surrender your hand and let your opponent win the pot

[State]
You are Player 0 (first to act this round).
Your card: 'J'
History: Player 0: [check] -> Player 1: [bet]
Available actions: [fold], [call]

[Output Format]
``` plaintext
<think> Your thoughts and reasoning </think>
\\boxed{[ACTION]}
```

[Important Notes]
1. You must always include the <think> field to outline your reasoning.
2. Your final action [ACTION] must be one of the available actions, in \\boxed{[ACTION]} format.
'''