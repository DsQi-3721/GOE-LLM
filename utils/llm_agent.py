from transformers import AutoTokenizer

from textarena.core import Agent
from textarena.agents.basic_agents import STANDARD_GAME_PROMPT

from vllm import LLM, SamplingParams

import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

MODEL_PATH = "/data/models/Qwen3-4B"

MODEL = LLM(model=MODEL_PATH, tensor_parallel_size=2)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

def post_processing(output: str) -> str:
    """
    Post-process the output from the LLM to ensure it is in the correct format.
    This function can be customized based on the specific requirements of the game.
    :param output: The raw output string from the LLM.
    :return: The processed output string.
    """
    #TODO: Implement specific post-processing logic as needed for the game.
    return output.strip()


def call_llm(messages: list[dict], sampling_params, thinking: bool = False) -> str:
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=thinking,
    )

    response = MODEL.generate(
        [text],
        sampling_params=sampling_params,
        use_tqdm=False
    )
    output = response[0].outputs[0].text
    return post_processing(output)


class VLLMAgent(Agent):
    def __init__(self, temperature: float = 0.7, top_p: float = 0.95, max_tokens: int = 512):
        super().__init__()
        self.system_prompt = STANDARD_GAME_PROMPT

        self.sampling_params = SamplingParams(
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens
        )

    def __call__(self, observation: str) -> str:
        """
        Call the agent with an observation and return the action.
        :param observation: The observation string from the environment.
        :return: The action string generated by the agent.
        """
        logger.debug(f"VLLMAgent Observation: {observation}")

        messages = [
            {"role": "system", "content": self.system_prompt}, 
            {"role": "user", "content": observation}
        ]
        response = call_llm(messages, self.sampling_params)

        logger.debug(f"VLLMAgent Action: {response}")

        return response
    
    def __str__(self):
        return "VLLMAgent"
    